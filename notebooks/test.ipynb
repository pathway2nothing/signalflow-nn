{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18caa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: Training TemporalValidator with SignalFlow-NN\n",
    "\n",
    "Flow:\n",
    "1. Load raw data\n",
    "2. Extract features (RAW values, normalization handled by preprocessor)\n",
    "3. Detect signals\n",
    "4. Label signals\n",
    "5. Configure Preprocessor & Train Validator\n",
    "6. Validate new signals\n",
    "\"\"\"\n",
    "import signalflow as sf\n",
    "from signalflow.nn.validator import TemporalValidator\n",
    "from signalflow.nn.model.temporal_classificator import TrainingConfig\n",
    "from signalflow.nn.data.ts_preprocessor import TimeSeriesPreprocessor, ScalerConfig  # <--- NEW IMPORT\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Load Raw Data\n",
    "# ============================================================================\n",
    "\n",
    "raw_data = sf.data.RawDataFactory.from_duckdb_spot_store(\n",
    "    spot_store_path=Path(\"test.duckdb\"),\n",
    "    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\", \"XRPUSDT\"],\n",
    "    start=datetime(2024, 1, 1),\n",
    "    end=datetime(2025, 12, 31),\n",
    "    data_types=[\"spot\"],\n",
    ")\n",
    "raw_data_view = sf.core.RawDataView(raw_data)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "feature_set = sf.feature.FeatureSet(extractors=[\n",
    "    sf.feature.pandasta.PandasTaRsiExtractor(length=14),\n",
    "    sf.feature.pandasta.PandasTaMacdExtractor(fast=12, slow=26, signal=9),\n",
    "    sf.feature.pandasta.PandasTaAtrExtractor(length=14),\n",
    "    sf.feature.pandasta.PandasTaBbandsExtractor(length=20, std=2.0),\n",
    "])\n",
    "features_df = feature_set.extract(raw_data_view)\n",
    "\n",
    "# Визначаємо список колонок (але НЕ нормалізуємо тут вручну)\n",
    "feature_cols = [c for c in features_df.columns if c not in [\"pair\", \"timestamp\"]]\n",
    "\n",
    "print(f\"Features shape: {features_df.shape} (full history, RAW values)\")\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols[:5]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Signal Detection\n",
    "# ============================================================================\n",
    "\n",
    "detector = sf.detector.SmaCrossSignalDetector(fast_period=10, slow_period=30)\n",
    "signals = detector.run(raw_data_view)\n",
    "\n",
    "# Filter to actionable signals only\n",
    "actionable_signals = signals.value.filter(\n",
    "    pl.col(\"signal_type\").is_in([\"rise\", \"fall\"])\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected {signals.value.height} total signals\")\n",
    "print(f\"Actionable signals: {actionable_signals.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Labeling\n",
    "# ============================================================================\n",
    "\n",
    "from signalflow.target import FixedHorizonLabeler\n",
    "\n",
    "labeler = FixedHorizonLabeler(\n",
    "    price_col=\"close\",\n",
    "    horizon=120,\n",
    "    out_col=\"label\",\n",
    "    include_meta=True,\n",
    ")\n",
    "\n",
    "spot_df = raw_data_view.to_polars(\"spot\")\n",
    "labeled_full = labeler.compute(spot_df)\n",
    "\n",
    "labeled_signals = (\n",
    "    actionable_signals\n",
    "    .select([\"pair\", \"timestamp\", \"signal_type\"])\n",
    "    .join(\n",
    "        labeled_full.select([\"pair\", \"timestamp\", \"label\"]),\n",
    "        on=[\"pair\", \"timestamp\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .filter(pl.col(\"label\").is_not_null())\n",
    ")\n",
    "\n",
    "labeled_signals = labeled_signals.with_columns(\n",
    "    pl.when(pl.col(\"label\") == \"rise\").then(1)\n",
    "    .when(pl.col(\"label\") == \"fall\").then(2)\n",
    "    .otherwise(0)\n",
    "    .cast(pl.Int64)\n",
    "    .alias(\"label\")\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeled signals: {labeled_signals.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Configure and Train Validator (WITH PREPROCESSOR)\n",
    "# ============================================================================\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "\n",
    "# Encoder config\n",
    "encoder_params = {\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"bidirectional\": False,\n",
    "}\n",
    "\n",
    "# Head config\n",
    "head_params = {\n",
    "    \"hidden_sizes\": [128, 64],\n",
    "    \"dropout\": 0.3,\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "# Training config\n",
    "training_config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"scheduler\": \"reduce_on_plateau\",\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"label_smoothing\": 0.1,\n",
    "}\n",
    "\n",
    "# --- NEW: Configure Preprocessor ---\n",
    "# Автоматична нормалізація (Robust) по кожній групі (парі) окремо\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    default_config=ScalerConfig(method=\"robust\", scope=\"group\"),\n",
    "    group_col=\"pair\",\n",
    "    time_col=\"timestamp\"\n",
    ")\n",
    "# -----------------------------------\n",
    "\n",
    "# Create validator\n",
    "validator = TemporalValidator(\n",
    "    encoder_type=\"encoder/gru\",\n",
    "    encoder_params=encoder_params,\n",
    "    head_type=\"head/cls/linear\",\n",
    "    head_params=head_params,\n",
    "    \n",
    "    preprocessor=preprocessor,  # <--- CONNECTED HERE\n",
    "    \n",
    "    window_size=96,\n",
    "    window_timeframe=15,\n",
    "    num_classes=3,\n",
    "    class_weights=[1.0, 1.0, 1.0],\n",
    "    training_config=training_config,\n",
    "    feature_cols=feature_cols,\n",
    "    max_epochs=100,\n",
    "    batch_size=256,\n",
    "    early_stopping_patience=5,  \n",
    "    train_val_test_split=(0.6, 0.2, 0.2),\n",
    "    split_strategy=\"temporal\",\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# fit() now handles scaling internally (fit on train -> transform all)\n",
    "validator.fit(\n",
    "    X_train=features_df,      \n",
    "    y_train=labeled_signals,    \n",
    "    log_dir=Path(\"./logs/temporal_validator\"),\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Validate New Signals\n",
    "# ============================================================================\n",
    "\n",
    "# Features are automatically scaled using the saved preprocessor state\n",
    "validated_signals = validator.validate_signals(signals, features_df)\n",
    "\n",
    "validated_df = validated_signals.value.with_columns([\n",
    "    pl.col(\"probability_none\").alias(\"prob_neutral\"),\n",
    "    pl.col(\"probability_rise\").alias(\"prob_rise\"),\n",
    "    pl.col(\"probability_fall\").alias(\"prob_fall\"),\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Analyze Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Top Rise Signals\n",
    "print(\"\\nTop Rise Signals (high probability):\")\n",
    "rise_signals = (\n",
    "    validated_df\n",
    "    .filter(pl.col(\"signal_type\") == \"rise\")\n",
    "    .sort(\"prob_rise\", descending=True)\n",
    "    .select([\"timestamp\", \"pair\", \"prob_rise\", \"prob_fall\", \"prob_neutral\"])\n",
    "    .head(10)\n",
    ")\n",
    "print(rise_signals)\n",
    "\n",
    "# High-confidence signals (>70% probability)\n",
    "HIGH_CONF_THRESHOLD = 0.7\n",
    "high_conf_rise = validated_df.filter(\n",
    "    (pl.col(\"signal_type\") == \"rise\") & (pl.col(\"prob_rise\") > HIGH_CONF_THRESHOLD)\n",
    ")\n",
    "\n",
    "print(f\"\\nHigh-confidence signals (>{HIGH_CONF_THRESHOLD*100:.0f}%):\")\n",
    "print(f\"  Rise signals: {high_conf_rise.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Save Validator\n",
    "# ============================================================================\n",
    "\n",
    "validator.save(Path(\"./best_models/temporal_validator.pkl\"))\n",
    "print(\"\\nValidator saved to ./models/temporal_validator.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a13db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: Training TemporalValidator with SignalFlow-NN\n",
    "\n",
    "Flow:\n",
    "1. Load raw data\n",
    "2. Extract features (RAW values, normalization handled by preprocessor)\n",
    "3. Detect signals\n",
    "4. Label signals\n",
    "5. Configure Preprocessor & Train Validator\n",
    "6. Validate new signals\n",
    "7. Calculate classification metrics\n",
    "\"\"\"\n",
    "import signalflow as sf\n",
    "from signalflow.nn.validator import TemporalValidator\n",
    "from signalflow.nn.model.temporal_classificator import TrainingConfig\n",
    "from signalflow.nn.data.ts_preprocessor import TimeSeriesPreprocessor, ScalerConfig\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Load Raw Data\n",
    "# ============================================================================\n",
    "\n",
    "raw_data = sf.data.RawDataFactory.from_duckdb_spot_store(\n",
    "    spot_store_path=Path(\"test.duckdb\"),\n",
    "    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\", \"XRPUSDT\"],\n",
    "    start=datetime(2022, 1, 1),\n",
    "    end=datetime(2025, 12, 31),\n",
    "    data_types=[\"spot\"],\n",
    ")\n",
    "raw_data_view = sf.core.RawDataView(raw_data)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "feature_set = sf.feature.FeatureSet(extractors=[\n",
    "    sf.feature.pandasta.PandasTaRsiExtractor(length=14),\n",
    "    sf.feature.pandasta.PandasTaMacdExtractor(fast=12, slow=26, signal=9),\n",
    "    sf.feature.pandasta.PandasTaAtrExtractor(length=14),\n",
    "    sf.feature.pandasta.PandasTaBbandsExtractor(length=20, std=2.0),\n",
    "])\n",
    "features_df = feature_set.extract(raw_data_view)\n",
    "\n",
    "feature_cols = [c for c in features_df.columns if c not in [\"pair\", \"timestamp\"]]\n",
    "\n",
    "print(f\"Features shape: {features_df.shape} (full history, RAW values)\")\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols[:5]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Signal Detection\n",
    "# ============================================================================\n",
    "\n",
    "detector = sf.detector.SmaCrossSignalDetector(fast_period=10, slow_period=30)\n",
    "signals = detector.run(raw_data_view)\n",
    "\n",
    "actionable_signals = signals.value.filter(\n",
    "    pl.col(\"signal_type\").is_in([\"rise\", \"fall\"])\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected {signals.value.height} total signals\")\n",
    "print(f\"Actionable signals: {actionable_signals.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Labeling\n",
    "# ============================================================================\n",
    "\n",
    "from signalflow.target import FixedHorizonLabeler\n",
    "\n",
    "labeler = FixedHorizonLabeler(\n",
    "    price_col=\"close\",\n",
    "    horizon=120,\n",
    "    out_col=\"label\",\n",
    "    include_meta=True,\n",
    ")\n",
    "\n",
    "spot_df = raw_data_view.to_polars(\"spot\")\n",
    "labeled_full = labeler.compute(spot_df)\n",
    "\n",
    "labeled_signals = (\n",
    "    actionable_signals\n",
    "    .select([\"pair\", \"timestamp\", \"signal_type\"])\n",
    "    .join(\n",
    "        labeled_full.select([\"pair\", \"timestamp\", \"label\"]),\n",
    "        on=[\"pair\", \"timestamp\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .filter(pl.col(\"label\").is_not_null())\n",
    ")\n",
    "\n",
    "labeled_signals = labeled_signals.with_columns(\n",
    "    pl.when(pl.col(\"label\") == \"rise\").then(1)\n",
    "    .when(pl.col(\"label\") == \"fall\").then(2)\n",
    "    .otherwise(0)\n",
    "    .cast(pl.Int64)\n",
    "    .alias(\"label\")\n",
    ")\n",
    "\n",
    "print(f\"\\nLabeled signals: {labeled_signals.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Configure and Train Validator (WITH PREPROCESSOR)\n",
    "# ============================================================================\n",
    "\n",
    "input_size = len(feature_cols)\n",
    "\n",
    "encoder_params = {\n",
    "    \"input_size\": input_size,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"bidirectional\": False,\n",
    "}\n",
    "\n",
    "head_params = {\n",
    "    \"hidden_sizes\": [128, 64],\n",
    "    \"dropout\": 0.3,\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"scheduler\": \"reduce_on_plateau\",\n",
    "    \"scheduler_patience\": 5,\n",
    "    \"label_smoothing\": 0.1,\n",
    "}\n",
    "\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    default_config=ScalerConfig(method=\"robust\", scope=\"group\"),\n",
    "    group_col=\"pair\",\n",
    "    time_col=\"timestamp\"\n",
    ")\n",
    "\n",
    "validator = TemporalValidator(\n",
    "    encoder_type=\"encoder/gru\",\n",
    "    encoder_params=encoder_params,\n",
    "    head_type=\"head/cls/linear\",\n",
    "    head_params=head_params,\n",
    "    \n",
    "    preprocessor=preprocessor,\n",
    "    \n",
    "    window_size=96,\n",
    "    window_timeframe=15,\n",
    "    num_classes=3,\n",
    "    class_weights=[1.0, 1.0, 1.0],\n",
    "    training_config=training_config,\n",
    "    feature_cols=feature_cols,\n",
    "    max_epochs=100,\n",
    "    batch_size=256,\n",
    "    early_stopping_patience=5,  \n",
    "    train_val_test_split=(0.6, 0.2, 0.2),\n",
    "    split_strategy=\"temporal\",\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "validator.fit(\n",
    "    X_train=features_df,      \n",
    "    y_train=labeled_signals,    \n",
    "    log_dir=Path(\"./logs/temporal_validator\"),\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Validate New Signals\n",
    "# ============================================================================\n",
    "\n",
    "validated_signals = validator.validate_signals(signals, features_df)\n",
    "\n",
    "validated_df = validated_signals.value.with_columns([\n",
    "    pl.col(\"probability_none\").alias(\"prob_neutral\"),\n",
    "    pl.col(\"probability_rise\").alias(\"prob_rise\"),\n",
    "    pl.col(\"probability_fall\").alias(\"prob_fall\"),\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Classification Metrics Calculation\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "def calculate_classification_metrics(\n",
    "    validated_df: pl.DataFrame,\n",
    "    labeled_signals: pl.DataFrame,\n",
    "    class_names: list[str] = [\"none\", \"rise\", \"fall\"],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        validated_df: DataFrame with predictions (prob_neutral, prob_rise, prob_fall)\n",
    "        labeled_signals: DataFrame with ground truth labels\n",
    "        class_names: List of class names for reporting\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    # Join predictions with ground truth\n",
    "    eval_df = (\n",
    "        validated_df\n",
    "        .select([\"pair\", \"timestamp\", \"prob_neutral\", \"prob_rise\", \"prob_fall\"])\n",
    "        .join(\n",
    "            labeled_signals.select([\"pair\", \"timestamp\", \"label\"]),\n",
    "            on=[\"pair\", \"timestamp\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if eval_df.height == 0:\n",
    "        print(\"Warning: No matching samples for evaluation\")\n",
    "        return {}\n",
    "    \n",
    "    # Track original count before filtering\n",
    "    n_total = eval_df.height\n",
    "    \n",
    "    # Filter out rows with NaN in probabilities or labels\n",
    "    eval_df = eval_df.filter(\n",
    "        pl.col(\"prob_neutral\").is_not_null() &\n",
    "        pl.col(\"prob_rise\").is_not_null() &\n",
    "        pl.col(\"prob_fall\").is_not_null() &\n",
    "        pl.col(\"label\").is_not_null() &\n",
    "        pl.col(\"prob_neutral\").is_not_nan() &\n",
    "        pl.col(\"prob_rise\").is_not_nan() &\n",
    "        pl.col(\"prob_fall\").is_not_nan()\n",
    "    )\n",
    "    \n",
    "    n_valid = eval_df.height\n",
    "    n_dropped = n_total - n_valid\n",
    "    \n",
    "    if n_dropped > 0:\n",
    "        print(f\"Warning: Dropped {n_dropped} samples with NaN values ({n_dropped/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if eval_df.height == 0:\n",
    "        print(\"Warning: No valid samples after filtering NaN values\")\n",
    "        return {}\n",
    "    \n",
    "    # Get predicted class (argmax of probabilities)\n",
    "    eval_df = eval_df.with_columns(\n",
    "        pl.concat_list([\"prob_neutral\", \"prob_rise\", \"prob_fall\"])\n",
    "        .list.arg_max()\n",
    "        .alias(\"pred_class\")\n",
    "    )\n",
    "    \n",
    "    # Extract arrays\n",
    "    y_true = eval_df.get_column(\"label\").to_numpy()\n",
    "    y_pred = eval_df.get_column(\"pred_class\").to_numpy()\n",
    "    \n",
    "    # Probability matrix for ROC-AUC\n",
    "    y_proba = eval_df.select([\"prob_neutral\", \"prob_rise\", \"prob_fall\"]).to_numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"balanced_accuracy\"] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    metrics[\"precision_macro\"] = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    metrics[\"recall_macro\"] = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    metrics[\"f1_macro\"] = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    metrics[\"precision_weighted\"] = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    metrics[\"recall_weighted\"] = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    metrics[\"f1_weighted\"] = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    \n",
    "    # Per-class detailed metrics\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    for i, name in enumerate(class_names):\n",
    "        if i < len(precision_per_class):\n",
    "            metrics[f\"precision_{name}\"] = precision_per_class[i]\n",
    "            metrics[f\"recall_{name}\"] = recall_per_class[i]\n",
    "            metrics[f\"f1_{name}\"] = f1_per_class[i]\n",
    "    \n",
    "    # ROC-AUC (one-vs-rest)\n",
    "    try:\n",
    "        unique_classes = np.unique(y_true)\n",
    "        if len(unique_classes) > 1:\n",
    "            metrics[\"roc_auc_ovr\"] = roc_auc_score(\n",
    "                y_true, y_proba, multi_class=\"ovr\", average=\"macro\"\n",
    "            )\n",
    "            metrics[\"roc_auc_ovo\"] = roc_auc_score(\n",
    "                y_true, y_proba, multi_class=\"ovo\", average=\"macro\"\n",
    "            )\n",
    "    except ValueError as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Classification report (string)\n",
    "    metrics[\"classification_report\"] = classification_report(\n",
    "        y_true, y_pred, target_names=class_names, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Sample counts\n",
    "    metrics[\"n_samples\"] = n_valid\n",
    "    metrics[\"n_dropped_nan\"] = n_dropped\n",
    "    metrics[\"nan_ratio\"] = n_dropped / n_total if n_total > 0 else 0.0\n",
    "    metrics[\"class_distribution\"] = {\n",
    "        int(k): int(v) for k, v in zip(*np.unique(y_true, return_counts=True))\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def print_metrics_report(metrics: dict, class_names: list[str] = [\"none\", \"rise\", \"fall\"]):\n",
    "    \"\"\"Pretty print classification metrics.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CLASSIFICATION METRICS REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nSamples evaluated: {metrics.get('n_samples', 'N/A')}\")\n",
    "    print(f\"Class distribution: {metrics.get('class_distribution', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"OVERALL METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Accuracy:          {metrics.get('accuracy', 0):.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {metrics.get('balanced_accuracy', 0):.4f}\")\n",
    "    print(f\"  MCC:               {metrics.get('mcc', 0):.4f}\")\n",
    "    print(f\"  ROC-AUC (OvR):     {metrics.get('roc_auc_ovr', 'N/A')}\")\n",
    "    print(f\"  ROC-AUC (OvO):     {metrics.get('roc_auc_ovo', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"MACRO-AVERAGED METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Precision: {metrics.get('precision_macro', 0):.4f}\")\n",
    "    print(f\"  Recall:    {metrics.get('recall_macro', 0):.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics.get('f1_macro', 0):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"WEIGHTED-AVERAGED METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Precision: {metrics.get('precision_weighted', 0):.4f}\")\n",
    "    print(f\"  Recall:    {metrics.get('recall_weighted', 0):.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics.get('f1_weighted', 0):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"PER-CLASS METRICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "    for name in class_names:\n",
    "        p = metrics.get(f\"precision_{name}\", 0)\n",
    "        r = metrics.get(f\"recall_{name}\", 0)\n",
    "        f1 = metrics.get(f\"f1_{name}\", 0)\n",
    "        print(f\"{name:<12} {p:<12.4f} {r:<12.4f} {f1:<12.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"CONFUSION MATRIX\")\n",
    "    print(\"-\" * 40)\n",
    "    cm = metrics.get(\"confusion_matrix\")\n",
    "    if cm is not None:\n",
    "        # Header\n",
    "        header = \"Pred →  \" + \"  \".join(f\"{name:>8}\" for name in class_names)\n",
    "        print(header)\n",
    "        print(\"True ↓\")\n",
    "        for i, name in enumerate(class_names):\n",
    "            row = f\"{name:<8}\" + \"  \".join(f\"{cm[i, j]:>8}\" for j in range(len(class_names)))\n",
    "            print(row)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"FULL CLASSIFICATION REPORT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(metrics.get(\"classification_report\", \"N/A\"))\n",
    "\n",
    "\n",
    "def calculate_metrics_by_signal_type(\n",
    "    validated_df: pl.DataFrame,\n",
    "    labeled_signals: pl.DataFrame,\n",
    ") -> dict:\n",
    "    \"\"\"Calculate metrics separately for rise and fall signals.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for signal_type in [\"rise\", \"fall\"]:\n",
    "        # Filter to specific signal type\n",
    "        signal_validated = validated_df.filter(pl.col(\"signal_type\") == signal_type)\n",
    "        signal_labels = labeled_signals.filter(pl.col(\"signal_type\") == signal_type)\n",
    "        \n",
    "        if signal_validated.height == 0 or signal_labels.height == 0:\n",
    "            continue\n",
    "            \n",
    "        metrics = calculate_classification_metrics(\n",
    "            signal_validated, \n",
    "            signal_labels,\n",
    "            class_names=[\"none\", \"rise\", \"fall\"]\n",
    "        )\n",
    "        results[signal_type] = metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CALCULATING CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metrics = calculate_classification_metrics(validated_df, labeled_signals)\n",
    "print_metrics_report(metrics)\n",
    "\n",
    "# Metrics by signal type\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRICS BY SIGNAL TYPE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metrics_by_type = calculate_metrics_by_signal_type(validated_df, labeled_signals)\n",
    "\n",
    "for signal_type, signal_metrics in metrics_by_type.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Signal Type: {signal_type.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"  Samples: {signal_metrics.get('n_samples', 0)}\")\n",
    "    print(f\"  Accuracy: {signal_metrics.get('accuracy', 0):.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {signal_metrics.get('balanced_accuracy', 0):.4f}\")\n",
    "    print(f\"  F1 (macro): {signal_metrics.get('f1_macro', 0):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. Analyze Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Top Rise Signals\n",
    "print(\"\\nTop Rise Signals (high probability):\")\n",
    "rise_signals = (\n",
    "    validated_df\n",
    "    .filter(pl.col(\"signal_type\") == \"rise\")\n",
    "    .sort(\"prob_rise\", descending=True)\n",
    "    .select([\"timestamp\", \"pair\", \"prob_rise\", \"prob_fall\", \"prob_neutral\"])\n",
    "    .head(10)\n",
    ")\n",
    "print(rise_signals)\n",
    "\n",
    "# High-confidence signals\n",
    "HIGH_CONF_THRESHOLD = 0.7\n",
    "high_conf_rise = validated_df.filter(\n",
    "    (pl.col(\"signal_type\") == \"rise\") & (pl.col(\"prob_rise\") > HIGH_CONF_THRESHOLD)\n",
    ")\n",
    "high_conf_fall = validated_df.filter(\n",
    "    (pl.col(\"signal_type\") == \"fall\") & (pl.col(\"prob_fall\") > HIGH_CONF_THRESHOLD)\n",
    ")\n",
    "\n",
    "print(f\"\\nHigh-confidence signals (>{HIGH_CONF_THRESHOLD*100:.0f}%):\")\n",
    "print(f\"  Rise signals: {high_conf_rise.height}\")\n",
    "print(f\"  Fall signals: {high_conf_fall.height}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Save Validator and Metrics\n",
    "# ============================================================================\n",
    "\n",
    "validator.save(Path(\"./best_models/temporal_validator.pkl\"))\n",
    "print(\"\\nValidator saved to ./models/temporal_validator.pkl\")\n",
    "\n",
    "# Save metrics to JSON\n",
    "import json\n",
    "\n",
    "metrics_to_save = {k: v for k, v in metrics.items() \n",
    "                   if k not in [\"confusion_matrix\", \"classification_report\"]}\n",
    "metrics_to_save[\"confusion_matrix\"] = metrics[\"confusion_matrix\"].tolist()\n",
    "\n",
    "# Fix: convert numpy int64 keys to regular int\n",
    "if \"class_distribution\" in metrics_to_save:\n",
    "    metrics_to_save[\"class_distribution\"] = {\n",
    "        int(k): int(v) for k, v in metrics_to_save[\"class_distribution\"].items()\n",
    "    }\n",
    "\n",
    "with open(\"./best_models/classification_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_to_save, f, indent=2, default=str)\n",
    "\n",
    "print(\"Metrics saved to ./best_models/classification_metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
